# 机器学习-期中考试

## 单选、多选、判断、简答

![image-20230414081356117](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20230414081356117.png)

### 深度学习的一般步骤？

- 数据处理：从本地或网络读取数据，并完成预处理操作，保证模型可读取
- 模型设计：网络结构设计
- 训练配置：设定模型采用的寻解算法，即优化器，并指定计算资源（配置超参数）
- 训练过程：循环调用训练过程，每轮都包括前向计算、损失函数（优化目标）和反向传播三个步骤
- 模型保存：将训练好的模型保存，模型预测时调用
- 模型部署应用

### 深度学习中的超参数：

- **learning rate**
- epochs(迭代次数，也可称为 num of iterations)
- num of hidden layers(隐层数目)
- num of hidden layer units(隐层的单元数/神经元数)
- **activation function**(激活函数)
- **batch-size**(用mini-batch SGD的时候每个批量的大小)
- **optimizer**(选择什么优化器，如SGD、RMSProp、Adam)
- 用诸如RMSProp、Adam优化器的时候涉及到的β1，β2等等

![img](https://pic4.zhimg.com/80/v2-764b9b4c1026f861f690d37a65786b17_720w.webp)

### 机器学习的三要素

机器学习的三要素：模型、策略、算法

### 机器学习中数据集的分类？

- 训练数据：用来训练模型，让模型能够拟合训练数据， 这个样本通常会大一些
- 验证数据：用来验证模型的预测准确率，通常不对验证 数据训练；不应与训练数据混在一起（ImageNet google）
- 测试数据：一般只用一次，要求测试数据完全没有出现 在训练数据中过，而且也绝对不能对测试数据进行训练。

### 机器学习中的Error

- 训练误差:训练过程中的误差 
- 验证误差:在验证集上进行交叉验证选择参数（调参），最终模型 在验证集上的误差 
- 测试误差:训练完毕、调参完毕的模型，在新的测试集上的误差 
- 泛化误差:假如所有数据来自一个整体，模型在这个整体上的误差。 通常说来，测试误差的平均值或者说期望就是泛化误差

> 训练误差 < 验证误差 < 测试误差 ～= 泛化误差 

### 模型训练过程中的一些概念（超参数）

- Epoch：一个epoch等于使用训练集中的全部样本训练一次的 过程。当一个完整的数据集通过了神经网络一次并且返回了 一次，即进行了一次正向传播和反向传播，这个过程称为一 个epoch。
- Batch：整个训练集分成多个小块，也就是就是分成多个Batch
- Iteration：训练一个batch的过程为一个iteration

### 简述模型选择的方法

- 交叉验证：留出法、留一法交叉验证、K折交叉验证
  - 很多时候数据不够充足，这种时候可以取消验证集，采用交 叉验证方法，通过反复划分训练集和测试集来避免用同一批数 据训练和评估一个模型，相当于将验证集和测试集合二为一了。
  - 留一法：每次只使用一个作为测试集，剩下的全部作为训练集。 这种方法得出的结果与训练整个测试集的期望值最为接近，但是成本过于庞大。![image-20230414085910002](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20230414085910002.png)
- 正则化：L1、L2正则化
  - ![image-20230414090029695](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20230414090029695.png)
  - ![image-20230414090041536](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20230414090041536.png)
  - dropout法：dropout是神经网络中常用的正则化方法，就是 在训练过程中，随机地“丢弃”一些神经元，强行 简化模型
  - ![image-20230414090122213](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20230414090122213.png)
  - 早停法
  - ![image-20230414090153800](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20230414090153800.png)

### 欠拟合与过拟合问题

- 欠拟合的算法会表现出Bias非常高，Variance非常低 
- 过拟合的算法会表现出Bias非常低，Variance非常高

![image-20230414090000400](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20230414090000400.png)

## 第一章节&第二章节

`有监督学习`：从给定的有标注的训练数据集中学习出一个函数（模型参数），当心的数据到来时可以根据这个函数预测结果。常见任务包括`分类与回归`。

`无监督学习`：没有标注的训练数据集，需要根据样本间的统计规律对样本集进行分析。有`主成分分析`、`因果关系和概率图模型问题`、`生成对抗性网络`等几类。

`半监督学习`：结合（少量的）标注训练数据和（大量的）未标注数据来进行数据的分类学习。其两个基本假设为`聚类假设`和`流形假设`。

`强化学习`：`外部环境对输出只给出评价信息而非正确答案`，学习机通过强化受奖励的动作来改善自身性能。

常用的机器（深度）学习框架：Tensorflow、Caffe、Theano、MXNet、Pytorch

![image-20230414081309256](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20230414081309256.png)

![image-20230414081316643](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20230414081316643.png)

![image-20230414081323000](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20230414081323000.png)

## 第三章

### 线性回归

回归（regression）是能为⼀个或多个⾃变量与因变量之间关系建模的⼀类⽅法。在⾃然科学和社会科学领 域，回归经常⽤来表⽰输⼊和输出之间的关系。

损失函数（loss function） 能够量化⽬标的实际值与预测值之间的差距。通常我们会选择⾮负数作为损失，且数值越⼩表⽰损失越⼩， 完美预测时的损失为0。回归问题中最常⽤的损失函数是平⽅误差函数。

当样本i的预测值为yˆ (i)，其相应的真 实标签为y (i)时，平⽅误差可以定义为以下公式：

![image-20230414081954270](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20230414081954270.png)

解析解  线性回归的解可 以⽤⼀个公式简单地表达出来，这类解叫作解析解（analytical solution）。

### 梯度下降

梯度就是定义一个函数在某一个点的全部偏导数构成的向量

![image-20230414082227145](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20230414082227145.png)

梯度下降常分为BGD（批量梯度下降）和SGD（随机梯度下降）

其中，又有MSGD（小批量随机梯度下降）

可能会考简答题：分析BGD、SGD、MSGD各自的优缺点

- BGD 批量梯度下降
  - 优点：![image-20230414082424207](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20230414082424207.png)
  - 缺点：![image-20230414082434409](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20230414082434409.png)
- SGD 随机梯度下降
  - 优点：![image-20230414082509557](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20230414082509557.png)
  - 缺点：![image-20230414082521594](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20230414082521594.png)
- MSGD 小批量随机梯度下降
  - 优点：![image-20230414082546500](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20230414082546500.png)
  - 缺点：![image-20230414082555580](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20230414082555580.png)

### 如果在线性回归的实现过程中，将权重初始化为零，会发生什么？算法仍然有效吗？

当将权重全部初始化为零时，算法仍然有效，但是会导致模型无法学习到数据中的任何模式，因为所有权重将始终保持为零。这意味着模型将始终输出固定的预测，损失函数将始终保持不变。因此，算法仍然有效，但是无法实现有意义的学习。

如果将损失函数定义为L1损失，即绝对误差，那么当权重全部初始化为零时，梯度将始终指向同一方向，即如果预测值小于真实值，则将权重向正方向调整；如果预测值大于真实值，则将权重向负方向调整。这样，权重将根据数据中的模式进行调整，但是这可能需要更长时间才能收敛到最优解。

### softmax回归

之前提到，回归问题的主要任务是预测及分类。

![image-20230414084459187](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20230414084459187.png)

Softmax函数或称归一化指数函数，是逻辑函数的一种推广，多个神经元输出映射到(0,1)区间内

Softmax函数一般作为神经网络的最后一层，接受来自上一层网络的输入值，然后将其转化为概率

**softmax 回归**(softmax regression)其实是 logistic 回归的一般形式，logistic 回归用于二分类，而 softmax 回归用于**多分类**

为了估计所有可能类别的条件概率，我们需要⼀个有多个输出的模型，每个类别对应⼀个输出。为了解决线 性模型的分类问题，我们需要和输出⼀样多的仿射函数（affine function）。每个输出对应于它⾃⼰的仿射函 数。

与线性回归⼀样，softmax回归也是⼀个单层神经⽹络。 由于计算每个输出o1、o2和o3取决于所有输⼊x1、x2、x3和x4，所以softmax回归的输出层也是全连接层。

![image-20230414083542789](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20230414083542789.png)

### softmax回归选择的损失函数

MSE或者交叉熵损失

交叉熵时梯度比较陡峭，利于做优化 

MSE 导致的梯度有很大的平坦区域，优化过程很可能卡住。

![image-20230414084829979](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20230414084829979.png)

### softmax回归是一个线性模型吗？为什么？

softmax运算不会改变未规范化 的预测o之间的⼤⼩次序，只会确定分配给每个类别的概率。因此，在预测过程中，我们仍然可以⽤下式来选 择最有可能的类别。

![image-20230414083635749](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20230414083635749.png)

尽管softmax是⼀个⾮线性函数，但softmax回归的输出仍然由输⼊特征的仿射变换决定。因此，softmax回 归是⼀个线性模型（linear model）。

![image-20230414083824759](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20230414083824759.png)

![image-20230414083848352](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20230414083848352.png)

### 熵

在信息论中，熵用来衡量一个随机事件的不确定性

信息熵 熵表示一个事件所包含的信息量

- 熵越高，则随机变量的信息越多 
- 熵越低，则随机变量的信息越少

KL散度是衡量两个概率分布的相似性的一个度量指标

### 逻辑回归

logistic回归用于分类，假设得到的类别为0或者1，使用sigmoid函 数处理输入，这个函数类似于阶跃函数但是又是连续型函数。

![image-20230414084254891](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20230414084254891.png)

## 第四章：感知机

激活函数为sign(x) 

感知机是人工神经网络的第一个实际应用，标志着神经网络进入 了新的发展阶段。

> Q：人工神经网络的第一个实际应用是什么？
>
> A：感知机

神经网络的感知机其实就是一个线 性方程再套上一个激活（励）函数， 是组成神经网络的基本单元。

### 讲讲激活函数的作用及特点？

- 激活函数是人工神经网络的一个极其重要的特征
- 激活函数决定一个神经元是否应该被激活，激活代表神经元接收的 信息与给定的信息有关
- 激活函数对输入信息进行非线性变换，然后将变换后的输出信息作 为输入信息传给下一层神经元。

激活函数的作用 

如果不用激活函数，每一层输出都是上层输入的线性函数，无论神经网 络有多少层，最终的输出都是输入的线性组合。 激活函数给神经元引 入了非线性因素，使得神经网络可以任意逼近任何非线性函数。

### 各种激活函数的优缺点：

![image-20230414085147184](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20230414085147184.png)

![image-20230414085203823](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20230414085203823.png)

![image-20230414085208538](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20230414085208538.png)

![image-20230414085217347](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20230414085217347.png)

![image-20230414085223875](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20230414085223875.png)

![image-20230414085230229](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20230414085230229.png)

![image-20230414085236094](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20230414085236094.png)

![image-20230414085241237](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20230414085241237.png)

### 选择激活函数的考虑要点

- 尽量选择输出具有zero-centered特点的激活函数以加快模型的 收敛速度
- 浅层网络在分类器时，sigmoid函数及其组合通常效果更好
- 由于梯度消失问题，有时要避免使用 sigmoid和 tanh函数
- relu函数是一个通用的激活函数，目前在大多数情况下使用
- relu函数只能在隐藏层中使用
- 如果使用 ReLU，那么一定要小心设置 learning rate，而且要注 意不要让网络出现很多 “dead” 神经元，可以试试 其他函数。

### 感知机不能学习什么问题？为什么？为了解决这一缺点提出了什么？

感知机无法学习 XOR 问题（神经元只能生成线性分离器）

由于无法模拟诸如异或以及其他复杂函数的功能，使得单层感 知机的应用较为单一。 

为了解决线性不可分问题，提出了多层感知机（基本都是两层， 因为BP算法提出之前只能训练一层网络）。

### 多层感知机

相邻层所包含的神经元之间通常使用“全连接”方式进 行连接。所谓“全连接”是指两个相邻层之间的神经元 相互成对连接，但同一层内神经元之间没有连接。

多层感知机可以模拟复杂非线性函数功能，让神经网络 具有更强拟合能力，所模拟函数的复杂性取决于网络隐 藏层数目和各层中神经元数目。

多层感知机在单 层神经网络的基 础上引入了一到 多个隐藏层 （hidden layer）。 隐藏层位于输入 层和输出层之间。

多层感知机就是含有至少一个隐藏层的 由全连接层组成的神经网络，且每个隐 藏层的输出通过激活函数进行变换。

![image-20230414085521960](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20230414085521960.png)

## 反向传播算法

训练一个神经网络可以分为两个步骤：

- 前向传播，计算损失函数。
- 反向传播，计算梯度，更新参数

![image-20230414090322152](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20230414090322152.png)

## 实验相关

### miniconda、cuda、pycharm、pytorch之间的关系是什么

- miniconda：miniconda是迷你化精简版的anaconda发行版，省去了一些冗余无关的环境包，提供了python虚拟环境的管理、安装、使用等功能
- CUDA：CUDA是NVIDIA英伟达发明的一种并行计算平台和变成模型，通过利用GPU大幅提升了算力和计算性能，只有英伟达显卡支持使用CUDA，而且不同的显卡所能兼容的CUDA版本号不同，在西在安装前需要先确认当前显卡是否支持CUDA
- pycharm：类似于C++可以使用命令行、VS Code、Visual Studio2019等工具进行开发，python语言同样可以使用轻量型编辑器、集成编辑编译环境等等，而Pycharm就是JetBrains发行的一款集成式Python环境，能够很方便的对Python代码进行编写、调试和使用
- d2l：是dive into Deep     Learning的语义缩写，d2l是李沐老师对于《动手学深度学习》中提供的代码使用的公共库，包含了三大类的可使用库：MXnet、PyTorch、TensorFlow
- PyTorch:是FaceBook人工智能研究院（FAIR）于2017年1月基于Torch推出的开源的Python机器学习库，提供了很多高级功能，是目前最流行的深度学习框架
- torchvision：torchvision是pytorch的一个图形库，其服务于Pytorch框架，主要用来构建计算机视觉模型。

![image-20230414090533357](https://happygoing.oss-cn-beijing.aliyuncs.com/img/image-20230414090533357.png)

### 训练增加迭代周期的数量。为什么测试精度会在一段时间后降低，属于什么现象？

训练增加迭代周期的数量会导致模型在训练集上的误差逐渐减小，但在测试集上的误差可能会先减小后增加，这被称为过拟合现象。

过拟合现象是由于模型在训练集上过于复杂，导致其在测试集上的泛化能力下降。为了解决这个问题，可以采用以下方法：

- 增加数据量：通过增加训练集和测试集的数据量来减少过拟合现象。
- 正则化：通过增加正则化项来限制模型的复杂度，从而减少过拟合现象。
- 早停法：在训练过程中，当测试集上的误差开始上升时，停止训练，从而避免过拟合现象。
- Dropout：在神经网络中加入Dropout层，随机将一部分神经元的输出置为0，从而减少过拟合现象。
- 数据增强：通过对训练集的数据进行旋转、平移、缩放等变换，增加数据的多样性，从而减少过拟合现象。

### 简略解释Relu、leakyRelu和pRelu的关系及优缺点。

Relu（Rectified Linear Unit）是一种神经网络激活函数，主要用于解决梯度消失问题。它的公式很简单：f(x)=max(0,x)，若x大于0，则f(x)=x，否则f(x)=0。

Leaky Relu是对于Relu的改进，主要解决relu函数中的缺点：对于负数部分输出为0，这类数据对计算结果没有影响，但是对于网络的权重更新造成麻烦。因此，Leaky Relu使用一个小的斜率来定义负数部分输出，即：f(x)= max(ax,x)。

PReLU是在Leaky ReLU的基础上进行了改进，它是带参数的ReLU，可以通过反向传播来进行参数优化调整，即： f(x)=max(0,x)+a*min(0,x)。 PReLU 的优化参数可以通过神经网络的优化来学习。

